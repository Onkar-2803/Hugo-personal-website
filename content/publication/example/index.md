---
title: Analyzing Architectures for Neural Machine Translation using Low
  Computational Resources
subtitle: ""
publication_types:
  - "2"
authors:
  - Aditya Mandke
  - Onkar Litake
  - and Dipali Kadam
publication_short: In IJNLC
abstract: >-
  With the recent developments in the field of Natural Language Processing,
  there has been a rise in the use of different architectures for Neural Machine
  Translation. Transformer architectures are used to achieve state-of-the-art
  accuracy, but they are very computationally expensive to train. Everyone
  cannot have such

  setups consisting of high-end GPUs and other resources. We train our models on low computational resources and investigate the results. As expected, transformers outperformed other architectures, but there were some surprising results. Transformers consisting of more encoders and decoders took more time to train but had fewer BLEU scores. LSTM performed well in the experiment and took comparatively less time to train than transformers, making it suitable to use in situations having time constraints.
draft: false
featured: false
projects: []
slides: ""
url_pdf: ""
summary: ""
url_dataset: ""
url_project: ""
url_source: ""
url_video: ""
author_notes:
  - Equal contribution
  - Equal contribution
  - Equal contribution
doi: ""
publication: In International Journal on Natural Language Computing
tags: []
image:
  caption: ""
  focal_point: ""
  preview_only: false
  filename: featured.png
date: 2021-11-26T18:15:33.176Z
url_slides: ""
publishDate: 2017-01-01T00:00:00.000Z
url_poster: ""
url_code: ""

links:
url_pdf: https://arxiv.org/ftp/arxiv/papers/2110/2110.05270.pdf
---
